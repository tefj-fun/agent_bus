apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-worker
  namespace: agent-bus
  labels:
    app: agent-bus
    component: gpu-worker
spec:
  backoffLimit: 3
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: agent-bus
        component: gpu-worker
    spec:
      restartPolicy: OnFailure
      containers:
      - name: gpu-worker
        image: agent_bus:latest
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: agent-bus-config
        - secretRef:
            name: agent-bus-secrets
        env:
        - name: WORKER_TYPE
          value: "gpu"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        command:
        - python
        - -m
        - src.workers.worker
        resources:
          requests:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "8000m"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: workspace
        persistentVolumeClaim:
          claimName: gpu-worker-workspace-pvc
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
      # GPU node affinity and tolerations
      nodeSelector:
        accelerator: nvidia-tesla-v100  # or nvidia-tesla-a100
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: "true"
        effect: NoSchedule
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gpu-worker-workspace-pvc
  namespace: agent-bus
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: standard
